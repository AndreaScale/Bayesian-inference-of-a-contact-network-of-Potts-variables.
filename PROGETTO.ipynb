{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given $N=5$ variables and $M$ observations. The observations are:\n",
    "\n",
    "$$\\textbf{x}^{(m)}=(x_1^{(m)},\\dots ,x_N^{(m)})$$\n",
    "\n",
    "Where $x_i^{(m)}\\in\\{1,\\dots,q\\}$ are categorical variable. In order to model the interaction between these variables we use the Potts model. Let $J_{ij}\\in \\R^{q\\times q}$ be the coupling matrix for the variables $i$ and $j$ having respectevly colors $a$ and $b$, i.e. if $J_{ij}(a,b)\\neq0$ then the variables interact (accordingly to the sign) while if $J_{ij}(a,b)\\sim0$ they do not. \n",
    "\n",
    "We consider the model:\n",
    "\n",
    "$$P\\left(x\\,|\\,J\\right)=\\frac{1}{Z}e^{\\sum_{ij}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i,a)}\\delta_{(x_j,b)}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$Z\\left(J\\right) = \\sum_{x} e^{\\sum_{i,j}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i,a)}\\delta_{(x_j,b)}}$$\n",
    "\n",
    "Therefore we consider the likelihood:\n",
    "\n",
    "$$\\mathcal{L}\\left(J;\\left\\{x_i^{(m)}\\right\\}_{m=1}^{M}\\right)=\\prod_m\\frac{1}{Z}e^{\\sum_{ij}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)}}$$\n",
    "\n",
    "In order to design a Boltzmann machine learing scheme let us compute the log-likelihood divided by $M$ and its derivative with respect to each parameter $J_{ij}(a,b)$:\n",
    "\n",
    "$$\\mathcal{l}\\left( J ; \\{x^{(m)} \\} \\right) = {\\frac{1}{M}}  \\sum_{ij}\\sum_{ab}\\sum_mJ_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} - \\log\\left[ Z\\left( J\\right)\\right]$$\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{l} \\left(J ; \\{x^{(m)} \\}  \\right)}{\\partial J_{i,j}(a,b)} & = {\\frac{1}{M}} \\sum_{m} \\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} - \\frac{1}{Z\\left( J\\right)} \\sum_{\\boldsymbol{x}} \\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} e^{\\sum_{i,j}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)}} \\\\\n",
    "& = { < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{data}} -  < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{model}}}\n",
    "\\end{align}\n",
    "\n",
    "Then by Boltzmann machine learning we can inferr the coupling matrices. The first term of the difference is easly computable, the second quite not. In order to reach our goal we can use a MCMC with Metropolis-Hastings update or Gibbs sampling:\n",
    "\n",
    "$$J_{i,j}^{t+1}(a,b) \\leftarrow J_{i,j}^{t}(a,b) + \\lambda_{J} \\left[ < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{data}} -  < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{model}\\left(t \\right)} \\right] $$\n",
    "\n",
    "For what concernes the MCMC with Metropolis-Hastings update:\n",
    "\n",
    "\n",
    "- We start from a uniformly randomly extracted configuration $\\boldsymbol{x}^{t = 0}$\n",
    "\n",
    "\n",
    "- As proposal distribution: extract an index $i\\in\\{1,\\dots,N\\}$ and a variable $a\\in\\{1,\\dots,q\\}$ and substitute the value of $\\delta_{(x_i^{t-1},a)}$ with its opposite ($1\\rightarrow0$, $0\\rightarrow1$)\n",
    "\n",
    "\n",
    "- Accept the move with probability:\n",
    "\n",
    "$$p_{\\rm acc} =\\min\\left[1, \\frac{q(x^{t-1}|x)\\tilde{\\pi}(x)}{q(x|x^{t-1})\\tilde{\\pi}(x^{t-1})} \\right]$$\n",
    "\n",
    "- Save many configuations and compute $<x_{i} x_{j}>_{\\mathrm{model}\\left(t\\right)}$ at $\\sim$ equilibrium \n",
    "\n",
    "We have to compute the acceptance ration in our setting. By the simmetry of the proposal distribution $q$ we have:\n",
    "\n",
    "$$\\frac{q(x^{t-1}|x)\\tilde{\\pi}(x)}{q(x|x^{t-1})\\tilde{\\pi}(x^{t-1})}=\\frac{\\tilde{\\pi}(x)}{\\tilde{\\pi}(x^{t-1})}$$\n",
    "\n",
    "We extract the index $i$ and the color $a$ (different from the actual color of $x_i$)and create the proposal $x^n$ as follows:\n",
    "\n",
    "$$x_k^n=\\begin{cases}\n",
    "x_k & k\\neq i \\\\\n",
    "a & k=i\n",
    "\\end{cases}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\tilde{\\pi}(x^n)}{\\tilde{\\pi}(x)}=\\frac{e^{\\sum_{kj}\\sum_{cb}J_{kj}(c,b)\\delta_{(x_k^n,c)}\\delta_{(x_j^n,b)}}}{e^{\\sum_{kj}\\sum_{cb}J_{kj}(c,b)\\delta_{(x_k,c)}\\delta_{(x_j,b)}}}=\\frac{e^{\\sum_{kj}J_{kj}(x_k^n,x_j^n)}}{e^{\\sum_{kj}J_{kj}(x_k,x_j)}}$$\n",
    "\n",
    "Then by definition of $x^n$ and the simmetry $J_{ij}(a,b)=J_{ji}(b,a)$ we have:\n",
    "\n",
    "$$\\sum_{k,j}J_{kj}(x_k^n,x_j^n)-\\sum_{k,j}J_{kj}(x_k,x_j)=\\sum_{k}J_{ki}(x_k^n,x_i^n)-\\sum_{k}J_{ki}(x_k,x_i)+\\sum_{j}J_{ij}(x_i^n,x_j^n)-\\sum_{j}J_{ij}(x_i,x_j)=$$\n",
    "$$2\\sum_{j\\neq i}\\left(J_{ij}(x_i^n,x_j^n)-J_{ij}(x_i,x_j)\\right)$$\n",
    "\n",
    "Finally assuming $J_{jj}=0 \\,\\forall j$ we get:\n",
    "\n",
    "$$\\frac{\\tilde{\\pi}(x^n)}{\\tilde{\\pi}(x)}=2\\sum_{j}\\left(J_{ij}(x_i^n,x_j^n)-J_{ij}(x_i,x_j)\\right)$$\n",
    "\n",
    "Finally update the parameters until convergence, when the maximum value of the gradient is smaller than a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metropolis_ising (generic function with 1 method)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function metropolis_ising(x::Vector{Int64}, J::Array{Matrix{Float64}})\n",
    "\n",
    "    N = length(x)\n",
    "\n",
    "    i = rand(1:N)       # draw uniformly an index\n",
    "    c = rand(1:q-1)     # draw uniformly a color different to the current one of x[i], this is equivalent to uniformly draw a number between 1 and q-1\n",
    "                        # and then add the drawn number to the current categorical state of x[i]\n",
    "    xi_new = mod1(x[i]+c, q)\n",
    "    ΔJ = 0\n",
    "    for j in 1:N\n",
    "        if j != i  #we exclude j=i because in the first matrix J we use the proposal  \n",
    "            ΔJ += 2.0*(J[i,j][xi_new, x[j]] - J[i,j][x[i], x[j]])   # argument of the exponential given by the ratio of the target distribution\n",
    "        end\n",
    "    end\n",
    "    ΔJ += (J[i,i][xi_new, xi_new] - J[i,i][x[i], x[i]])\n",
    "    if rand() < exp(ΔJ)\n",
    "        x[i] = xi_new    # if the move is accepted, we update it\n",
    "    end\n",
    "    return x\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "compute_stats (generic function with 1 method)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function compute_stats(data::Matrix{Int64}; q = 4)\n",
    "    M = size(data, 1)\n",
    "    N = size(data, 2)\n",
    "\n",
    "    sij = Array{Matrix{Float64}}(undef, N, N) \n",
    "    for i in eachindex(sij)\n",
    "        sij[i] = zeros(Float64, q, q)\n",
    "    end\n",
    "\n",
    "    for i in 1:N, j in 1:N\n",
    "        for m in 1:M\n",
    "            sij[i,j][data[m,i], data[m,j]] += 1\n",
    "        end\n",
    "        sij[i,j] ./= M\n",
    "        #sij[j,i] .= sij[i,j]'\n",
    "    end\n",
    "\n",
    "    return sij  # this gives us the sample frequencies\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×5 Matrix{Int64}:\n",
       " 2  4  1  1  4\n",
       " 2  1  1  2  2\n",
       " 4  1  4  4  3\n",
       " 1  4  3  1  2\n",
       " 4  2  2  2  3\n",
       " 3  3  2  2  3\n",
       " 4  1  3  4  2\n",
       " 2  4  3  4  1\n",
       " 4  1  3  4  3\n",
       " 2  4  2  4  4\n",
       " 4  1  2  2  4\n",
       " 2  3  3  1  2\n",
       " 4  2  4  4  3\n",
       " ⋮           \n",
       " 4  1  1  2  3\n",
       " 4  3  3  4  3\n",
       " 2  3  4  1  3\n",
       " 2  4  4  1  3\n",
       " 2  1  1  4  3\n",
       " 4  2  1  1  4\n",
       " 2  4  3  1  4\n",
       " 1  4  4  4  1\n",
       " 4  1  1  2  3\n",
       " 3  4  3  2  4\n",
       " 3  3  1  4  4\n",
       " 3  4  2  1  4"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "df = DataFrame(CSV.File(\"C:\\\\Users\\\\lucia\\\\OneDrive\\\\Desktop\\\\Documenti\\\\Collegio Carlo Alberto\\\\Models and Algorithms\\\\data.dat\", delim = \" \", header = false))\n",
    "select!(df, Not( :Column6))\n",
    "xdata = Matrix{Int64}(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Matrix{Float64}}:\n",
       " [0.2555 0.0 0.0 0.0; 0.0 0.2545 0.0 0.0; 0.0 0.0 0.2345 0.0; 0.0 0.0 0.0 0.2555]                              …  [0.074 0.0605 0.0575 0.0635; 0.067 0.07 0.0535 0.064; 0.055 0.0565 0.068 0.055; 0.0655 0.057 0.063 0.07]\n",
       " [0.03 0.0735 0.056 0.083; 0.072 0.0315 0.072 0.073; 0.0855 0.069 0.032 0.0695; 0.068 0.0805 0.0745 0.03]         [0.067 0.06 0.056 0.0595; 0.064 0.0605 0.0595 0.0645; 0.068 0.063 0.0635 0.0615; 0.0625 0.0605 0.063 0.067]\n",
       " [0.025 0.075 0.0615 0.086; 0.0755 0.03 0.074 0.071; 0.077 0.0755 0.0275 0.071; 0.078 0.074 0.0715 0.0275]        [0.023 0.078 0.0655 0.081; 0.078 0.0255 0.0745 0.0725; 0.081 0.0695 0.0295 0.071; 0.0795 0.071 0.0725 0.028]\n",
       " [0.069 0.053 0.0575 0.0595; 0.0585 0.073 0.0595 0.0655; 0.0645 0.0685 0.0555 0.05; 0.0635 0.06 0.062 0.0805]     [0.025 0.0685 0.068 0.0775; 0.0855 0.0275 0.07 0.0735; 0.0755 0.067 0.023 0.073; 0.0755 0.081 0.081 0.0285]\n",
       " [0.074 0.067 0.055 0.0655; 0.0605 0.07 0.0565 0.057; 0.0575 0.0535 0.068 0.063; 0.0635 0.064 0.055 0.07]         [0.2615 0.0 0.0 0.0; 0.0 0.244 0.0 0.0; 0.0 0.0 0.242 0.0; 0.0 0.0 0.0 0.2525]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sij = compute_stats(xdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "boltzmann_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ProgressMeter\n",
    "using Distributions\n",
    "\n",
    "function boltzmann_learning(sij::Array{Matrix{Float64}}, J::Array{Matrix{Float64}}; \n",
    "                            λ::Float64 = 0.1, Tmax::Int64 = 500, Teq::Int = 500,\n",
    "                            Twait::Int = 100, dmax::Int64 = 500, εmax::Float64 = 1e-2)\n",
    "\n",
    "    N = size(sij, 1)\n",
    "    q = size(sij[1], 1)\n",
    "\n",
    "    xall = zeros(Int64, dmax, N)\n",
    "    x = sample(1:q, N, replace = true)   # the initial configuration is a random sequence of numbers between 1 and q\n",
    "\n",
    "    sij_model = Array{Matrix{Float64}}(undef, N, N) \n",
    "    for i in eachindex(sij_model)\n",
    "        sij_model[i] = zeros(Float64, q, q)\n",
    "    end\n",
    "\n",
    "    t = 0\n",
    "    ε = 1\n",
    "\n",
    "    ProgressMeter.ijulia_behavior(:clear)\n",
    "    p = ProgressUnknown(\"Learning...\")\n",
    "    while t <= Tmax && ε > εmax     # time associated with learning epochs. \n",
    "                                    # ϵ is a parameter checking whether I have reached convergence or not\n",
    "        t += 1\n",
    "        fill!(xall, 0)\n",
    "        x = sample(1:q, N, replace = true)\n",
    "\n",
    "        for d in 1:Teq  # time associated with the equilibration of the MC        \n",
    "            x = metropolis_ising(x, J)     # we update the initial random configuration using metropolis_ising. The matrix J that is used in the call\n",
    "                                            # of the function is exactly what we want to estimate and that is initialised by the programmer as he\n",
    "                                            # prefers.\n",
    "        end\n",
    "\n",
    "        for d in 1:dmax     # dmax is the number of samples of my Markov Chain that I want to store in order to estimate the J matrix of the model\n",
    "                            # although dmax may be different with respect to the number of samples that we have, it is advisable to use a dmax\n",
    "                            # that is similar to the total number of samples that we have\n",
    "            for d1 in 1:Twait\n",
    "                x = metropolis_ising(x, J)\n",
    "            end\n",
    "            xall[d,:] = x\n",
    "        end \n",
    "\n",
    "        sij_model = compute_stats(xall)    # now compute_stats takes as input our configurations, hence it returns the frequencies of the model\n",
    "        Δsij = sij .- sij_model\n",
    "        J .= J + λ .* (Δsij)    # J is updated using the gradient acend scheme\n",
    "        \n",
    "        ε = 0.0\n",
    "        for i in eachindex(Δsij)\n",
    "            ε_new = maximum(abs.(Δsij[i]))\n",
    "            if ε_new > ε\n",
    "                ε = ε_new\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #ε = maximum.([abs.(Δsij[i]) for i in eachindex(Δsij)])     # to check whether we have reached convergence or not\n",
    "        if mod(t, 10) == 0\n",
    "            ProgressMeter.next!(p; showvalues = [(:ε , ε)])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ProgressMeter.finish!(p)\n",
    "\n",
    "    return J, sij_model, ε, xall\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "q = 4\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N) \n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mLearning... 50 \t Time: 0:01:06\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[-0.00024999999999991755 0.0 0.0 0.0; 0.0 0.008650000000000019 0.0 0.0; 0.0 0.0 -0.009750000000000736 0.0; 0.0 0.0 0.0 0.0013500000000001432] [-0.3074000000000002 0.09199999999999979 0.18055000000000018 0.034600000000000374; 0.10874999999999987 -0.30645000000000017 0.06150000000000021 0.14485000000000012; 0.015400000000000035 0.11979999999999964 -0.27759999999999957 0.13264999999999977; 0.16230000000000047 0.11389999999999975 0.043750000000000275 -0.3186000000000003] … [0.025900000000000197 -0.05134999999999986 0.04785000000000001 -0.022650000000000003; -0.0597000000000001 0.050699999999999815 0.04805000000000032 -0.03040000000000008; -0.006049999999999906 0.02294999999999984 -0.04065000000000001 0.013999999999999957; 0.020949999999999847 -0.01224999999999992 -0.07399999999999989 0.06665000000000021] [0.012599999999999776 -0.004150000000000131 -0.026049999999999955 0.017350000000000015; 0.010900000000000206 0.025000000000000355 -0.037850000000000134 0.010600000000000075; -0.04290000000000002 0.023650000000000043 0.04440000000000028 -0.0349; 0.01775000000000014 -0.04389999999999999 0.015300000000000018 0.012200000000000344]; [-0.3074000000000002 0.10874999999999987 0.015400000000000035 0.16230000000000047; 0.09199999999999979 -0.30645000000000017 0.11979999999999964 0.11389999999999975; 0.18055000000000018 0.06150000000000021 -0.27759999999999957 0.043750000000000275; 0.034600000000000374 0.14485000000000012 0.13264999999999977 -0.3186000000000003] [-0.020950000000000454 0.0 0.0 0.0; 0.0 0.019249999999999774 0.0 0.0; 0.0 0.0 0.0082000000000001 0.0; 0.0 0.0 0.0 -0.006500000000000032] … [-0.37894999999999956 0.14540000000000006 0.09230000000000008 0.12029999999999984; 0.1387499999999999 -0.31685000000000013 0.0575500000000001 0.13980000000000017; 0.12474999999999986 0.10794999999999988 -0.34394999999999964 0.11944999999999997; 0.09655000000000026 0.07354999999999981 0.1753500000000002 -0.35195000000000004] [-0.01569999999999981 0.01599999999999986 -0.017600000000000032 -0.0036500000000001826; 0.03519999999999998 -0.030550000000000035 -0.013650000000000176 0.028250000000000018; 0.03220000000000025 0.01349999999999995 -0.02764999999999998 -0.009850000000000064; -0.05334999999999999 0.0016499999999998586 0.05470000000000011 -0.009499999999999833]; … ; [0.025900000000000197 -0.0597000000000001 -0.006049999999999906 0.020949999999999847; -0.05134999999999986 0.050699999999999815 0.02294999999999984 -0.01224999999999992; 0.04785000000000001 0.04805000000000032 -0.04065000000000001 -0.07399999999999989; -0.022650000000000003 -0.03040000000000008 0.013999999999999957 0.06665000000000021] [-0.37894999999999956 0.1387499999999999 0.12474999999999986 0.09655000000000026; 0.14540000000000006 -0.31685000000000013 0.10794999999999988 0.07354999999999981; 0.09230000000000008 0.0575500000000001 -0.34394999999999964 0.1753500000000002; 0.12029999999999984 0.13980000000000017 0.11944999999999997 -0.35195000000000004] … [-0.018900000000000573 0.0 0.0 0.0; 0.0 0.010050000000000102 0.0 0.0; 0.0 0.0 -0.01875000000000059 0.0; 0.0 0.0 0.0 0.02760000000000065] [-0.39990000000000037 0.1242500000000003 0.10180000000000024 0.15495000000000012; 0.17495000000000036 -0.3684499999999994 0.1100000000000003 0.09354999999999974; 0.14875000000000013 0.0923000000000002 -0.39889999999999987 0.13910000000000006; 0.07454999999999971 0.1525000000000004 0.18290000000000012 -0.3823499999999991]; [0.012599999999999776 0.010900000000000206 -0.04290000000000002 0.01775000000000014; -0.004150000000000131 0.025000000000000355 0.023650000000000043 -0.04389999999999999; -0.026049999999999955 -0.037850000000000134 0.04440000000000028 0.015300000000000018; 0.017350000000000015 0.010600000000000075 -0.0349 0.012200000000000344] [-0.01569999999999981 0.03519999999999998 0.03220000000000025 -0.05334999999999999; 0.01599999999999986 -0.030550000000000035 0.01349999999999995 0.0016499999999998586; -0.017600000000000032 -0.013650000000000176 -0.02764999999999998 0.05470000000000011; -0.0036500000000001826 0.028250000000000018 -0.009850000000000064 -0.009499999999999833] … [-0.39990000000000037 0.17495000000000036 0.14875000000000013 0.07454999999999971; 0.1242500000000003 -0.3684499999999994 0.0923000000000002 0.1525000000000004; 0.10180000000000024 0.1100000000000003 -0.39889999999999987 0.18290000000000012; 0.15495000000000012 0.09354999999999974 0.13910000000000006 -0.3823499999999991] [-0.001649999999999636 0.0 0.0 0.0; 0.0 0.0005999999999995484 0.0 0.0; 0.0 0.0 -0.004200000000000451 0.0; 0.0 0.0 0.0 0.005249999999999955]], [[0.252 0.0 0.0 0.0; 0.0 0.294 0.0 0.0; 0.0 0.0 0.234 0.0; 0.0 0.0 0.0 0.22] [0.018 0.076 0.07 0.088; 0.088 0.038 0.074 0.094; 0.05 0.082 0.034 0.068; 0.084 0.066 0.052 0.018] … [0.07 0.054 0.072 0.056; 0.072 0.078 0.074 0.07; 0.06 0.052 0.06 0.062; 0.052 0.064 0.036 0.068] [0.062 0.064 0.064 0.062; 0.084 0.086 0.066 0.058; 0.052 0.06 0.054 0.068; 0.07 0.05 0.052 0.048]; [0.018 0.088 0.05 0.084; 0.076 0.038 0.082 0.066; 0.07 0.074 0.034 0.052; 0.088 0.094 0.068 0.018] [0.24 0.0 0.0 0.0; 0.0 0.262 0.0 0.0; 0.0 0.0 0.23 0.0; 0.0 0.0 0.0 0.268] … [0.03 0.08 0.068 0.062; 0.084 0.028 0.058 0.092; 0.062 0.062 0.026 0.08; 0.078 0.078 0.09 0.022] [0.07 0.068 0.062 0.04; 0.06 0.068 0.068 0.066; 0.072 0.068 0.05 0.04; 0.066 0.056 0.056 0.09]; … ; [0.07 0.072 0.06 0.052; 0.054 0.078 0.052 0.064; 0.072 0.074 0.06 0.036; 0.056 0.07 0.062 0.068] [0.03 0.084 0.062 0.078; 0.08 0.028 0.062 0.078; 0.068 0.058 0.026 0.09; 0.062 0.092 0.08 0.022] … [0.254 0.0 0.0 0.0; 0.0 0.248 0.0 0.0; 0.0 0.0 0.242 0.0; 0.0 0.0 0.0 0.256] [0.02 0.076 0.084 0.074; 0.092 0.038 0.05 0.068; 0.078 0.062 0.034 0.068; 0.078 0.084 0.068 0.026]; [0.062 0.084 0.052 0.07; 0.064 0.086 0.06 0.05; 0.064 0.066 0.054 0.052; 0.062 0.058 0.068 0.048] [0.07 0.06 0.072 0.066; 0.068 0.068 0.068 0.056; 0.062 0.068 0.05 0.056; 0.04 0.066 0.04 0.09] … [0.02 0.092 0.078 0.078; 0.076 0.038 0.062 0.084; 0.084 0.05 0.034 0.068; 0.074 0.068 0.068 0.026] [0.268 0.0 0.0 0.0; 0.0 0.26 0.0 0.0; 0.0 0.0 0.236 0.0; 0.0 0.0 0.0 0.236]], 0.03949999999999998, [4 3 … 4 1; 2 4 … 3 2; … ; 3 1 … 3 4; 1 4 … 3 2])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, sij_model, ε, xall = boltzmann_learning(sij, J, λ = 0.1, Tmax = 500, εmax = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03949999999999998"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frobenius (generic function with 1 method)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Frobenius(J::Array{Matrix{Float64}})\n",
    "    N = size(J,1)\n",
    "    q = size(J[1], 1)\n",
    "\n",
    "    Fij = zeros(Float64, N, N)\n",
    "    for i in 1:N, j in 1:N\n",
    "        for a in 1:q, b in 1:q\n",
    "            Fij[i,j] += (J[i,j][a,b])^2\n",
    "        end\n",
    "        Fij[i,j] = sqrt(Fij[i,j])\n",
    "    end\n",
    "    return Fij\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×5 Matrix{Float64}:\n",
       " 0.0131061  0.720581   0.83785   0.168513   0.107975\n",
       " 0.720581   0.0303143  0.127857  0.812016   0.109518\n",
       " 0.83785    0.127857   0.014546  0.106865   0.888788\n",
       " 0.168513   0.812016   0.106865  0.0396426  0.902507\n",
       " 0.107975   0.109518   0.888788  0.902507   0.00694874"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fij = Frobenius(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
