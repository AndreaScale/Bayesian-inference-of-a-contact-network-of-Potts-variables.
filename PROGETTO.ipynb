{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given $N=5$ variables and $M$ observations. The observations are:\n",
    "\n",
    "$$\\textbf{x}^{(m)}=(x_1^{(m)},\\dots ,x_N^{(m)})$$\n",
    "\n",
    "Where $x_i^{(m)}\\in\\{1,\\dots,q\\}$ are categorical variable. In order to model the interaction between these variables we use the Potts model. Let $J_{ij}\\in \\R^{q\\times q}$ be the coupling matrix for the variables $i$ and $j$ having respectevly colors $a$ and $b$, i.e. if $J_{ij}(a,b)\\neq0$ then the variables interact (accordingly to the sign) while if $J_{ij}(a,b)\\sim0$ they do not. \n",
    "\n",
    "We consider the model:\n",
    "\n",
    "$$P\\left(x\\,|\\,J\\right)=\\frac{1}{Z}e^{\\sum_{ij}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i,a)}\\delta_{(x_j,b)}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$Z\\left(J\\right) = \\sum_{x} e^{\\sum_{i,j}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i,a)}\\delta_{(x_j,b)}}$$\n",
    "\n",
    "Therefore we consider the likelihood:\n",
    "\n",
    "$$\\mathcal{L}\\left(J;\\left\\{x_i^{(m)}\\right\\}_{m=1}^{M}\\right)=\\prod_m\\frac{1}{Z}e^{\\sum_{ij}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)}}$$\n",
    "\n",
    "In order to design a Boltzmann machine learing scheme let us compute the log-likelihood divided by $M$ and its derivative with respect to each parameter $J_{ij}(a,b)$:\n",
    "\n",
    "$$\\mathcal{l}\\left( J ; \\{x^{(m)} \\}_{m=1}^{M} \\right) = {\\frac{1}{M}}  \\sum_{ij}\\sum_{ab}\\sum_m J_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} - \\log\\left[ Z\\left( J\\right)\\right]$$\n",
    "\n",
    "Then:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{l} \\left(J ; \\{x^{(m)} \\}_{m=1}^{M}  \\right)}{\\partial J_{i,j}(a,b)} & = {\\frac{1}{M}} \\sum_{m} \\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} - \\frac{1}{Z\\left( J\\right)} \\sum_{\\boldsymbol{x}} \\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)} e^{\\sum_{i,j}\\sum_{ab}J_{ij}(a,b)\\delta_{(x_i^{(m)},a)}\\delta_{(x_j^{(m)},b)}} \\\\\n",
    "& = { < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{data}} -  < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{model}}}\n",
    "\\end{align}\n",
    "\n",
    "Then by Boltzmann machine learning we can inferr the coupling matrices. The first term of the difference is easly computable, the second quite not. In order to reach our goal we can use a MCMC with Metropolis-Hastings update or Gibbs sampling:\n",
    "\n",
    "$$J_{i,j}^{t+1}(a,b) \\leftarrow J_{i,j}^{t}(a,b) + \\lambda_{J} \\left[ < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{data}} -  < \\delta_{(x_i,a)} \\delta_{(x_j,b)} >_{\\mathrm{model}\\left(t \\right)} \\right] $$\n",
    "\n",
    "For what concernes the MCMC with Metropolis-Hastings update:\n",
    "\n",
    "\n",
    "- We start from a uniformly randomly extracted configuration $\\boldsymbol{x}^{t = 0}$\n",
    "\n",
    "\n",
    "- As proposal distribution: extract an index $i\\in\\{1,\\dots,N\\}$ and a variable $a\\in\\{1,\\dots,q\\}$ and substitute the value of $\\delta_{(x_i^{t-1},a)}$ with its opposite ($1\\rightarrow0$, $0\\rightarrow1$)\n",
    "\n",
    "\n",
    "- Accept the move with probability:\n",
    "\n",
    "$$p_{\\rm acc} =\\min\\left[1, \\frac{q(x^{t-1}|x)\\tilde{\\pi}(x)}{q(x|x^{t-1})\\tilde{\\pi}(x^{t-1})} \\right]$$\n",
    "\n",
    "- Save many configuations and compute $<x_{i} x_{j}>_{\\mathrm{model}\\left(t\\right)}$ at $\\sim$ equilibrium \n",
    "\n",
    "We have to compute the acceptance ration in our setting. By the simmetry of the proposal distribution $q$ we have:\n",
    "\n",
    "$$\\frac{q(x^{t-1}|x)\\tilde{\\pi}(x)}{q(x|x^{t-1})\\tilde{\\pi}(x^{t-1})}=\\frac{\\tilde{\\pi}(x)}{\\tilde{\\pi}(x^{t-1})}$$\n",
    "\n",
    "We extract the index $i$ and the color $a$ (different from the actual color of $x_i$) and create the proposal $x^n$ as follows:\n",
    "\n",
    "$$x_k^n=\\begin{cases}\n",
    "x_k & k\\neq i \\\\\n",
    "a & k=i\n",
    "\\end{cases}$$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\frac{\\tilde{\\pi}(x^n)}{\\tilde{\\pi}(x)}=\\frac{e^{\\sum_{kj}\\sum_{cb}J_{kj}(c,b)\\delta_{(x_k^n,c)}\\delta_{(x_j^n,b)}}}{e^{\\sum_{kj}\\sum_{cb}J_{kj}(c,b)\\delta_{(x_k,c)}\\delta_{(x_j,b)}}}=\\frac{e^{\\sum_{kj}J_{kj}(x_k^n,x_j^n)}}{e^{\\sum_{kj}J_{kj}(x_k,x_j)}}$$\n",
    "\n",
    "Then by definition of $x^n$ and the simmetry $J_{ij}(a,b)=J_{ji}(b,a)$ we have:\n",
    "\n",
    "$$\\sum_{k,j}J_{kj}(x_k^n,x_j^n)-\\sum_{k,j}J_{kj}(x_k,x_j)=\\sum_{k}J_{ki}(x_k^n,x_i^n)-\\sum_{k}J_{ki}(x_k,x_i)+\\sum_{j}J_{ij}(x_i^n,x_j^n)-\\sum_{j}J_{ij}(x_i,x_j)=$$\n",
    "$$2\\sum_{j\\neq i}\\left(J_{ij}(x_i^n,x_j^n)-J_{ij}(x_i,x_j)\\right)$$\n",
    "\n",
    "Finally assuming $J_{jj}=0 \\,\\forall j$ we get:\n",
    "\n",
    "$$\\frac{\\tilde{\\pi}(x^n)}{\\tilde{\\pi}(x)}=2\\sum_{j}\\left(J_{ij}(x_i^n,x_j^n)-J_{ij}(x_i,x_j)\\right)$$\n",
    "\n",
    "Finally update the parameters until convergence, when the maximum value of the gradient is smaller than a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function metropolis_ising(x::Vector{Int64}, J::Array{Matrix{Float64}})\n",
    "\n",
    "    N = length(x)\n",
    "\n",
    "    i = rand(1:N)       # draw uniformly an index\n",
    "    c = rand(1:q-1)     # draw uniformly a color different to the current one of x[i], this is equivalent to uniformly draw a number between 1 and q-1\n",
    "                        # and then add the drawn number to the current categorical state of x[i]\n",
    "    xi_new = mod1(x[i]+c, q)\n",
    "    ΔJ = 0\n",
    "    for j in 1:N\n",
    "        if j != i  #we exclude j=i because in the first matrix J we use the proposal  \n",
    "            ΔJ += 2.0*(J[i,j][xi_new, x[j]] - J[i,j][x[i], x[j]])   # argument of the exponential given by the ratio of the target distribution\n",
    "        end\n",
    "    end\n",
    "    ΔJ += (J[i,i][xi_new, xi_new] - J[i,i][x[i], x[i]])\n",
    "    if rand() < exp(ΔJ)\n",
    "        x[i] = xi_new    # if the move is accepted, we update it\n",
    "    end\n",
    "    return x\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function compute_stats(data::Matrix{Int64}, M::Int64; q = 4) #M is the number of observations considered\n",
    "    N = size(data, 2)\n",
    "    if M>size(data, 1)\n",
    "        print(\"Too large M\")\n",
    "    else\n",
    "        sij = Array{Matrix{Float64}}(undef, N, N) \n",
    "        for i in eachindex(sij)\n",
    "            sij[i] = zeros(Float64, q, q)\n",
    "        end\n",
    "\n",
    "        for i in 1:N, j in 1:N\n",
    "            for m in 1:M\n",
    "                sij[i,j][data[m,i], data[m,j]] += 1\n",
    "            end\n",
    "            sij[i,j] ./= M\n",
    "            #sij[j,i] .= sij[i,j]'\n",
    "        end\n",
    "\n",
    "        return sij  # this gives us the sample frequencies\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "\n",
    "df = DataFrame(CSV.File(\"C:\\\\Users\\\\loren_1hne11w\\\\Documents\\\\Models&Algorithms\\\\data.dat\", delim = \" \", header = false))\n",
    "select!(df, Not( :Column6))\n",
    "xdata = Matrix{Int64}(df)\n",
    "#\"C:\\\\Users\\\\lucia\\\\OneDrive\\\\Desktop\\\\Documenti\\\\Collegio Carlo Alberto\\\\Models and Algorithms\\\\data.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = size(xdata, 1)\n",
    "sij = compute_stats(xdata, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ProgressMeter\n",
    "using Distributions\n",
    "\n",
    "function boltzmann_learning(sij::Array{Matrix{Float64}}, J::Array{Matrix{Float64}}; \n",
    "                            λ::Float64 = 0.1, Tmax::Int64 = 500, Teq::Int = 500,\n",
    "                            Twait::Int = 100, dmax::Int64 = 2000, εmax::Float64 = 1e-2)\n",
    "\n",
    "    N = size(sij, 1)\n",
    "    q = size(sij[1], 1)\n",
    "\n",
    "    xall = zeros(Int64, dmax, N)\n",
    "    x = sample(1:q, N, replace = true)   # the initial configuration is a random sequence of numbers between 1 and q\n",
    "\n",
    "    sij_model = Array{Matrix{Float64}}(undef, N, N) \n",
    "    for i in eachindex(sij_model)\n",
    "        sij_model[i] = zeros(Float64, q, q)\n",
    "    end\n",
    "\n",
    "    t = 0\n",
    "    ε = 1\n",
    "\n",
    "    ProgressMeter.ijulia_behavior(:clear)\n",
    "    p = ProgressUnknown(\"Learning...\")\n",
    "    while t <= Tmax && ε > εmax     # time associated with learning epochs. \n",
    "                                    # ϵ is a parameter checking whether I have reached convergence or not\n",
    "        t += 1\n",
    "        fill!(xall, 0)\n",
    "        x = sample(1:q, N, replace = true)\n",
    "\n",
    "        for d in 1:Teq  # time associated with the equilibration of the MC        \n",
    "            x = metropolis_ising(x, J)     # we update the initial random configuration using metropolis_ising. The matrix J that is used in the call\n",
    "                                            # of the function is exactly what we want to estimate and that is initialised by the programmer as he\n",
    "                                            # prefers.\n",
    "        end\n",
    "\n",
    "        for d in 1:dmax     # dmax is the number of samples of my Markov Chain that I want to store in order to estimate the J matrix of the model\n",
    "                            # although dmax may be different with respect to the number of samples that we have, it is advisable to use a dmax\n",
    "                            # that is similar to the total number of samples that we have\n",
    "            for d1 in 1:Twait\n",
    "                x = metropolis_ising(x, J)\n",
    "            end\n",
    "            xall[d,:] = x\n",
    "        end \n",
    "\n",
    "        sij_model = compute_stats(xall, M)    # now compute_stats takes as input our configurations, hence it returns the frequencies of the model\n",
    "        Δsij = sij .- sij_model\n",
    "        J .= J + λ .* (Δsij)    # J is updated using the gradient acend scheme\n",
    "        \n",
    "        ε = 0.0\n",
    "        for i in eachindex(Δsij)\n",
    "            ε_new = maximum(abs.(Δsij[i]))\n",
    "            if ε_new > ε\n",
    "                ε = ε_new\n",
    "            end\n",
    "        end\n",
    "\n",
    "        #ε = maximum.([abs.(Δsij[i]) for i in eachindex(Δsij)])     # to check whether we have reached convergence or not\n",
    "        if mod(t, 10) == 0\n",
    "            ProgressMeter.next!(p; showvalues = [(:ε , ε)])\n",
    "        end\n",
    "    end\n",
    "\n",
    "    ProgressMeter.finish!(p)\n",
    "\n",
    "    return J, sij_model, ε, xall\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "q = 4\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N) \n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J, sij_model, ε, xall = boltzmann_learning(sij, J, λ = 0.1, Tmax = 500, εmax = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ε"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Frobenius(J::Array{Matrix{Float64}})\n",
    "    N = size(J,1)\n",
    "    q = size(J[1], 1)\n",
    "\n",
    "    Fij = zeros(Float64, N, N)\n",
    "    for i in 1:N, j in 1:N\n",
    "        for a in 1:q, b in 1:q\n",
    "            Fij[i,j] += (J[i,j][a,b])^2\n",
    "        end\n",
    "        Fij[i,j] = sqrt(Fij[i,j])\n",
    "    end\n",
    "    return Fij\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fij = Frobenius(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "groundtruth = DataFrame(CSV.File(\"groundtruth.dat\", delim = \" \", header = false))\n",
    "F = Matrix{Int64}(groundtruth)\n",
    "f_true=zeros(5,5)\n",
    "for i in 1:5\n",
    "    f_true[i,F[i,1]] = 1\n",
    "    f_true[i,F[i,2]] = 1\n",
    "end\n",
    "f_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GraphRecipes\n",
    "using NetworkLayout\n",
    "using Plots\n",
    "\n",
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = Any[]\n",
    "for i in 1:size(Fij,1)\n",
    "    for j in i+1:size(Fij,2)\n",
    "        if Fij[i,j]>0.7\n",
    "            push!(Z, [i,j])\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LaTeXStrings\n",
    "p = Any[]\n",
    "for i in 1:size(Z,1)\n",
    "    g = heatmap(J[Z[i][1], Z[i][2]], xlabel = L\"%$(Z[i][1])\", ylabel = L\"%$(Z[i][2])\", colorbar_title = L\"J_{ij}\")\n",
    "    push!(p,g)\n",
    "end\n",
    "plot(p[1], p[2], aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(p[3], p[4], aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncorrelated = heatmap(J[5,1], xlabel = L\"4\", ylabel = L\"3\", colorbar_title = L\"J\")\n",
    "plot(p[5], uncorrelated, aspect_ratio = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to repeat the inference changing the number of configurations $M$ we recall the function that defines the Boltzmann machine and modify it restraining the number of observations from the file \"data.dat\". This procedure is actually straighforward given the definition that we gave to the function \"compute_stats\", in fact we just need to choose the number \"M\" as we want and then run the Boltzmann machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sij1 = compute_stats(xdata, 1)\n",
    "sij2 = compute_stats(xdata, 10)\n",
    "sij3 = compute_stats(xdata, 100)\n",
    "sij4 = compute_stats(xdata, 500)\n",
    "sij5 = compute_stats(xdata, 1000)\n",
    "\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N)\n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n",
    "\n",
    "J1, sij_model1, ε1, xall1 = boltzmann_learning(sij1, J, λ = 0.1, Tmax = 500, εmax = 1e-2)\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N)\n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n",
    "\n",
    "J2, sij_model2, ε2, xall2 = boltzmann_learning(sij2, J, λ = 0.1, Tmax = 500, εmax = 1e-2)\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N)\n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n",
    "\n",
    "J3, sij_model3, ε3, xall3 = boltzmann_learning(sij3, J, λ = 0.1, Tmax = 500, εmax = 1e-2)\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N)\n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n",
    "\n",
    "J4, sij_model4, ε4, xall4 = boltzmann_learning(sij4, J, λ = 0.1, Tmax = 500, εmax = 1e-2)\n",
    "\n",
    "J = Array{Matrix{Float64}}(undef, N, N)\n",
    "for i in eachindex(J)\n",
    "    J[i] = zeros(Float64, q, q)\n",
    "end\n",
    "\n",
    "J5, sij_model5, ε5, xall5 = boltzmann_learning(sij5, J, λ = 0.1, Tmax = 500, εmax = 1e-2)\n",
    "\n",
    "Fij = Frobenius(J1)\n",
    "Fij2 = Frobenius(J2)\n",
    "Fij3 = Frobenius(J3)\n",
    "Fij4 = Frobenius(J4)\n",
    "Fij5 = Frobenius(J5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using GraphRecipes\n",
    "using NetworkLayout\n",
    "using Plots\n",
    "\n",
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij2, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij3, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij4, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = graphplot(f_true, edgewidth = abs.(f_true), names = 1:N, method = :chorddiagram, title = \"True graph\")\n",
    "p2 = graphplot(Fij5, edgewidth = abs.(Fij), names = 1:N, method = :chorddiagram, title = \"Inferred graph\")\n",
    "\n",
    "plot(p1, p2, aspect_ratio = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{yellow}{\\text{I'm having troubles with VSC and therefore I cannot run the code. Anyways I think that the only thing we can comment about this point is that the less observations we consider the less accurate is our inference, and it should be clear from the Frobenius norms and the corresponding graphs (those with the lines that connect each color). What would you add?}}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main problems of MCMC is that the samples are not independent. Therefore usually the collection of samples is done waiting a time interval $\\Delta t$ that brings independece between samples. In order to quantify this correlation we estimate the autocorrelation function associated to the Markov chain that fetures the Bolzmann Machine. We recall that the autocorrelation function has the form:\n",
    "\\begin{align}\n",
    "\\gamma(r) = \\frac{\\mathbb{E}[(x^{(t)}-\\mu_t)(x^{(s)}-\\mu_s)]}{\\sqrt {\\sigma_t^2\\sigma_s^2}}\n",
    "\\end{align}\n",
    "Because of the stationarity of the Markov chain we get:\n",
    "\\begin{align}\n",
    "\\gamma(r) = \\frac{\\mathbb{E}[(x^{(t)}-\\mu)(x^{(t+r)}-\\mu)]}{\\sigma^2}\n",
    "\\end{align} \n",
    "As seen in the lecture an unbiased Monte Carlo estimator of $\\gamma$ is given by:\n",
    "\\begin{align}\n",
    "\\hat{\\gamma}_n(r) = \\frac{\\frac{1}{n-r}\\sum_{k=1}^{n-r}(\\hat{x}^{(k)}-\\hat{\\mu}_n)(\\hat{x}^{(k+r)}-\\hat{\\mu}_n)}{\\frac{1}{n}\\sum_{k=1}^{n}(\\hat{x}^{(k)}-\\hat{\\mu}_n)^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function autocorrelation(x::Array(Int64), R::Int64)\n",
    "    n = lenght(x)\n",
    "    μ = sum(x) / n\n",
    "    gamma = [0.0 for i in 1:R]\n",
    "\n",
    "    for r in 1:R\n",
    "        for i in 1:(n-r+1)\n",
    "            gamma[r] += (x[i] - μ) * (x[i+r-1] - μ) / (n - r + 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    var = gamma[1]\n",
    "    gamma ./= var\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{yellow}{\\text{In order to vary both M and }\\Delta t \\text{ we could construct a matrix of xall. Also, I don't know which R to use. We then inferr the autocorrelation by passing to \"autocorrelation\" the xall of the matrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
